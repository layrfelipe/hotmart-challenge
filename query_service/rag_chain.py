from pydantic import BaseModel, Field
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_ollama.llms import OllamaLLM
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA

from constants import CHROMA_DB_PERSIST_DIRECTORY

class RAGException(Exception):
    """Custom exception for RAG system errors"""
    pass

class QueryResponse(BaseModel):
    """Response model for RAG queries"""
    answer: str = Field(
        ...,
        description="The answer generated by the RAG system",
        example="A Hotmart é uma plataforma que permite a criação e venda de produtos digitais."
    )

class QueryRequest(BaseModel):
    """Request model for RAG queries"""
    question: str = Field(
        ...,
        min_length=1,
        max_length=500,
        description="The question to be answered by the RAG system",
        example="Como funciona a Hotmart?"
    )

class HotmartRAGSystem:
    def __init__(self):
        """Initialize the RAG system with necessary components"""
        try:
            self.llm = OllamaLLM(base_url="http://ollama:11434", model="mistral", temperature=0.3)
            self.embeddings = HuggingFaceEmbeddings(model_name="intfloat/multilingual-e5-small")
            
            self.vector_store = Chroma(
                persist_directory=CHROMA_DB_PERSIST_DIRECTORY,
                embedding_function=self.embeddings,
            )
            
            self.retriever = self.vector_store.as_retriever(
                search_kwargs={"k": 4},
                search_type="similarity"
            )
            
            self.prompt_template = """Responda a pergunta em português e com base no contexto fornecido.
                Se não souber, não invente.

                Por favor, traga o contexto que lhe foi oferecido também.

                Contexto: {context}
                Pergunta: {question}"""
        except Exception as e:
            raise RAGException(f"Failed to initialize RAG system: {str(e)}")
    
    def generate_response(self, question: str) -> dict:
        """
        Generate a response using the RAG system
        
        Args:
            question (str): The question to be answered
            
        Returns:
            dict: Contains the generated answer
            
        Raises:
            RAGException: If there's an error during response generation
        """
        try:
            if not question or not isinstance(question, str):
                raise RAGException("Invalid question format")

            prompt = PromptTemplate(
                template=self.prompt_template, 
                input_variables=["context", "question"]
            )
            
            qa_chain = RetrievalQA.from_chain_type(
                llm=self.llm,
                chain_type="stuff",
                retriever=self.retriever,
                chain_type_kwargs={"prompt": prompt}
            )
            
            result = qa_chain.invoke({"query": question})
            
            if not result or "result" not in result:
                raise RAGException("No valid response generated")
                
            return {
                "answer": result["result"]
            }
            
        except RAGException:
            raise
        except Exception as e:
            raise RAGException(f"Error generating response: {str(e)}")